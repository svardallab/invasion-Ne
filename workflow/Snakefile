NUM_CHROMOSOMES = 25
# Workaround CALCUA VSC requirements about conda environments and containers
COMMON = "calcua.sh"


localrules:
    all,
    agglomerate_tables,
    ballpark_ne


wildcard_constraints:
    seed="\d+",


rule all:
    input:
        expand(
            "steps/inference/{model}/constant_recent_past/n{pop}/s{seed}.nc",
            seed=range(100, 106),
           pop=[5000, 10000, 15000],
           model=["constant_model", "constant_piecewise_model"],
       ),
        expand(
           "steps/inference/{model}/exponential_growth/ne1_{ne1}_ne2_{ne2}_t{t_inv}/s{seed}.nc",
           seed=range(100, 106),
           ne1=[10000],
           ne2=[10, 100, 1000],
           t_inv=[50, 25, 75],
           model=[
               "constant_model",
               "constant_piecewise_model",
               "exponential_piecewise_model",
           ],
        ),
        expand(
           "steps/ibdne/constant_recent_past/n{pop}/s{seed}.ne",
           seed=range(100, 106),
           pop=[5000, 10000, 15000],
        ),
        expand(
            "steps/ibdne/exponential_growth/ne1_{ne1}_ne2_{ne2}_t{t_inv}/s{seed}.ne",
            seed=range(100, 106),
            ne1=[10000],
            ne2=[10, 100, 1000],
            t_inv=[50, 25, 75],
        ),
        expand(
            "steps/gone2/exponential_growth/ne1_{ne1}_ne2_{ne2}_t{t_inv}/s{seed}_GONE2_Ne",
            seed=range(100, 106),
            ne1=[10000],
            ne2=[10, 100, 1000],
            t_inv=[50, 25, 75],
        ),
        expand(
            "steps/gone2/constant_recent_past/n{pop}/s{seed}_GONE2_Ne",
            seed=range(100, 106),
            pop=[5000, 10000, 15000],
        ),
        expand(
            "steps/hapne/exponential_growth/ne1_{ne1}_ne2_{ne2}_t{t_inv}/s{seed}_{method}_hapne_estimate.csv",
            seed=range(100, 106),
            ne1=[10000],
            ne2=[10, 100, 1000],
            t_inv=[50, 25, 75],
            method=["ld", "ibd"],
        ),
        expand(
            "steps/hapne/constant_recent_past/n{pop}/s{seed}_{method}_hapne_estimate.csv",
            seed=range(100, 106),
            pop=[5000, 10000, 15000],
            method=["ld", "ibd"],
        ),


rule agglomerate_tables:
    input:
        expand("steps/{{prefix}}/s{{seed}}_chr{i}.csv", i=range(NUM_CHROMOSOMES)),
    output:
        "steps/{prefix}/s{seed}.csv",
    shell:
        """
        cat {input} > {output}
        """


rule measure_ld:
    input:
        "steps/bcfs/{prefix}/s{seed}_chr{i}.bcf",
        "steps/bcfs/{prefix}/s{seed}_chr{i}.bcf.csi",
    output:
        temp("steps/binned_ld/{prefix}/s{seed}_chr{i}.csv"),
    resources:
        mem_mb=2000,
        runtime="30min",
    envmodules:
        "calcua/2024a",
        "Clang/18.1.8-GCCcore-13.3.0",
    params:
        epsilon=0.001,
    shell:
        """
        external/ld_binning {input[0]} --seed {wildcards.seed} --epsilon {params.epsilon} > {output}
        """


rule tree_into_bcf:
    input:
        "steps/trees/{prefix}/s{seed}_chr{i}.trees",
    output:
        "steps/bcfs/{prefix}/s{seed}_chr{i}.bcf",
        "steps/bcfs/{prefix}/s{seed}_chr{i}.bcf.csi",
    resources:
        mem_mb=3000,
        runtime="30min",
    conda:
        "../external/conda_env.yaml"
    shell:
        """
        source {COMMON}
        python -m tskit vcf --allow-position-zero --contig-id chr{wildcards.i} {input} \
        | bcftools view -e 'POS=0' -O b > {output[0]}
        bcftools index {output[0]}
        """

rule ballpark_ne:
    input:
        script="src/utils/ballpark_ne.py",
        files=expand(
            "steps/trees/{{prefix}}_chr{i}.trees",
            i=range(NUM_CHROMOSOMES),
        ),
    output:
        "steps/inference/ballpark_ne/{prefix}.csv"
    params:
        mutation_rate=1e-8,
        recombination_rate=1e-8
    shell:
        """
        source {COMMON}
        python {input.script} {params.mutation_rate} {params.recombination_rate} {input.files} > {output}
        """

rule tree_into_vcf:
    input:
        "steps/trees/{prefix}/s{seed}_chr{i}.trees",
    output:
        "steps/vcfs/{prefix}/s{seed}_chr{i}.vcf.gz",
        "steps/vcfs/{prefix}/s{seed}_chr{i}.vcf.gz.tbi",
    resources:
        mem_mb=3000,
        runtime="30min",
    conda:
        "../external/conda_env.yaml"
    shell:
        """
        source {COMMON}
        python -m tskit vcf --contig-id chr{wildcards.i} {input} | bgzip -c > {output[0]}
        tabix -p vcf {output[0]}
        """


rule sim_constant_recent_past:
    input:
        "src/slim/constant_recent_past.py",
    output:
        expand(
            "steps/trees/constant_recent_past/n{{n}}/s{{seed}}_chr{i}.trees",
            i=range(NUM_CHROMOSOMES),
        ),
    resources:
        mem_mb=3000,
        runtime="120min",
    conda:
        "../external/conda_env.yaml"
    log:
        "logs/trees/constant_recent_past/n{n}/s{seed}.log",
    shell:
        """
        source {COMMON}
        python {input} {wildcards.seed} {wildcards.n} {output} 2> {log}
        """


rule sim_exponential_growth:
    input:
        "src/slim/exponential_growth.py",
    output:
        expand(
            "steps/trees/exponential_growth/ne1_{{ne1}}_ne2_{{ne2}}_t{{t_inv}}/s{{seed}}_chr{i}.trees",
            i=range(NUM_CHROMOSOMES),
        ),
    resources:
        mem_mb=3000,
        runtime="120min",
    conda:
        "../external/conda_env.yaml"
    log:
        "logs/trees/exponential_growth/ne1_{ne1}_ne2_{ne2}_t{t_inv}/s{seed}.log",
    shell:
        """
        source {COMMON}
        python {input} {wildcards.seed} {wildcards.ne1} {wildcards.ne2} {wildcards.t_inv} {output} 2> {log}
        """


# Fit constant population size
rule fit_constant_population_size:
    input:
        "src/pymc/constant_population_nuts.py",
        "steps/binned_ld/{prefix}/s{seed}.csv",
    output:
        "steps/inference/constant_model/{prefix}/s{seed}.nc",
    resources:
        mem_mb=2000,
        runtime="10min",
    threads: 2
    conda:
        "../external/conda_env.yaml"
    log:
        "logs/inference/constant_model/{prefix}/s{seed}.log",
    params:
        prior_mean=20_000,
        prior_sd=10_000,
        sample_size=200,
    shell:
        """
        source {COMMON}
        TMP_COMPILEDIR=$(mktemp -d)
        export PYTENSOR_FLAGS="compiledir=${{TMP_COMPILEDIR}}"
        python {input} {params.prior_mean} {params.prior_sd} {params.sample_size} {output} 2>&1 | tee {log}
        rm -rf "${{TMP_COMPILEDIR}}"
        """


# Fit piecewise constant population
rule fit_constant_piecewise_model:
    input:
        "src/pymc/constant_population_piecewise_nuts.py",
        "steps/binned_ld/{prefix}/s{seed}.csv",
    output:
        "steps/inference/constant_piecewise_model/{prefix}/s{seed}.nc",
    resources:
        mem_mb=2000,
        runtime="1h",
    threads: 4
    conda:
        "../external/conda_env.yaml"
    log:
        "logs/inference/constant_piecewise_model/{prefix}/s{seed}.log",
    params:
        ne_prior_mean=20_000,
        ne_prior_sd=10_000,
        t0_prior_mean=50,
        t0_prior_sd=30,
        sample_size=200,
    shell:
        """
        source {COMMON}
        TMP_COMPILEDIR=$(mktemp -d)
        export PYTENSOR_FLAGS="compiledir=${{TMP_COMPILEDIR}}"
        python {input} {params.ne_prior_mean} {params.ne_prior_sd} \
            {params.t0_prior_mean} {params.t0_prior_sd} \
            {params.sample_size} {output} 2>&1 | tee {log}
        rm -rf "${{TMP_COMPILEDIR}}"
        """


rule fit_exponential_piecewise_model:
    input:
        "src/pymc/exponential_piecewise_nuts.py",
        "steps/binned_ld/{prefix}/s{seed}.csv",
        "steps/inference/ballpark_ne/{prefix}/s{seed}.csv"
    output:
        "steps/inference/exponential_piecewise_model/{prefix}/s{seed}.nc",
    resources:
        mem_mb=2000,
        runtime="4h",
    threads: 4
    conda:
        "../external/conda_env.yaml"
    log:
        "logs/inference/exponential_piecewise_model/{prefix}/s{seed}.log",
    params:
        ne1_prior_mean=20_000,
        ne1_prior_sd=10_000,
        t0_prior_mean=50,
        t0_prior_sd=30,
        alpha_logfold_prior_sd=1,
        sample_size=200,
    shell:
        """
        source {COMMON}
        TMP_COMPILEDIR=$(mktemp -d)
        export PYTENSOR_FLAGS="compiledir=${{TMP_COMPILEDIR}}"
        python {input} {params.ne1_prior_mean} {params.ne1_prior_sd} \
            {params.t0_prior_mean} {params.t0_prior_sd} {params.alpha_logfold_prior_sd} \
            {params.sample_size} {wildcards.seed} {output} 2>&1 | tee {log}
        rm -rf "${{TMP_COMPILEDIR}}"
        """

rule fit_exponential_piecewise_nuts_fixed:
    input:
        "src/pymc/exponential_piecewise_nuts_fixed.py",
        "steps/binned_ld/{prefix}/s{seed}.csv",
        "steps/inference/ballpark_ne/{prefix}/s{seed}.csv"
    output:
        "steps/inference/exponential_piecewise_nuts_fixed/{prefix}/s{seed}.nc",
    resources:
        mem_mb=2000,
        runtime="4h",
    threads: 4
    conda:
        "../external/conda_env.yaml"
    log:
        "logs/inference/exponential_piecewise_nuts_fixed/{prefix}/s{seed}.log",
    params:
        ne1_prior_mean=20_000,
        ne1_prior_sd=10_000,
        t0_prior_mean=50,
        t0_prior_sd=30,
        alpha_logfold_prior_sd=1,
        sample_size=200,
    shell:
        """
        source {COMMON}
        TMP_COMPILEDIR=$(mktemp -d)
        export PYTENSOR_FLAGS="compiledir=${{TMP_COMPILEDIR}}"
        python {input} {params.ne1_prior_mean} {params.ne1_prior_sd} \
            {params.t0_prior_mean} {params.t0_prior_sd} {params.alpha_logfold_prior_sd} \
            {params.sample_size} {wildcards.seed} {output} 2>&1 | tee {log}
        rm -rf "${{TMP_COMPILEDIR}}"
        """

# Run GONE2
rule gone2:
    input:
        inbcfs=expand(
            "steps/bcfs/{{prefix}}/s{{seed}}_chr{i}.bcf", i=range(NUM_CHROMOSOMES)
        ),
        executable="external/gone2",
    output:
        estimate="steps/gone2/{prefix}/s{seed}_GONE2_Ne",
        d2="steps/gone2/{prefix}/s{seed}_GONE2_d2",
        stats="steps/gone2/{prefix}/s{seed}_GONE2_STATS",
    log:
        "logs/gone2/{prefix}/s{seed}.log",
    resources:
        runtime="2h",
    threads: 8
    params:
        rate=1.0,
        # We randomly subsample the chromosomes
        # Default maximum number of SNPs is 2e6, in the issue below the authors argue
        # no more than 50000 SNPs are needed
        # https://github.com/esrud/GONE/issues/40
        # Take 50_000 random SNPs per chromosome
        max_loci_per_chr=50000,
    shadow:
        "shallow"
    envmodules:
        "calcua/2024a",
        "foss/2024a",
        "BCFtools/1.21-GCC-13.3.0",
        "parallel/20240722-GCCcore-13.3.0"
    shell:
        """
        exec &> {log}

        echo "Processing chromosomes in parallel..."
        echo "{input.inbcfs}" | tr ' ' '\\n' | parallel --jobs {threads} '
            chr_bcf={{}} ;
            chr_id=$(basename $chr_bcf | sed "s/.*chr\\([0-9XY]\\+\\).bcf/\\1/") ;
            echo "Processing chromosome $chr_id" ;
            bcftools index $chr_bcf ;
            bcftools query -f "%CHROM\\t%POS\\n" $chr_bcf > snps_$chr_id.txt ;
            shuf --random-source=<(yes {wildcards.seed}) \\
                -n {params.max_loci_per_chr} snps_$chr_id.txt | \\
                awk "{{print \\$1 \\"\\t\\" \\$2}}" > regions_$chr_id.txt ;
            bcftools view -R regions_$chr_id.txt -O v -o chr_$chr_id.vcf $chr_bcf ;
            rm snps_$chr_id.txt regions_$chr_id.txt ;
            echo "Finished chromosome $chr_id"
        '
        echo "Concatenating VCF files..."
        bcftools concat chr_*.vcf -O v -o out.vcf
        rm chr_*.vcf
        echo "Running GONE2..."
        {input.executable} -S {wildcards.seed} -r {params.rate} -t {threads} out.vcf
        echo "Moving output files..."
        mv *_GONE2_Ne {output.estimate}
        mv *_GONE2_d2 {output.d2}
        mv *_GONE2_STATS {output.stats}
        rm out.vcf
        echo "Done"
        """
rule ibd_ne:
    input:
        inbcfs=expand(
            "steps/bcfs/{{prefix}}/s{{seed}}_chr{i}.bcf", i=range(NUM_CHROMOSOMES)
        ),
        executable_hapibd="external/hap-ibd.jar",
        executable_postprocess="external/merge-ibd-segments.jar",
        executable_ibdne="external/ibdne.jar",
        plink_map_script="src/utils/plink_map.py",
    output:
        ibds="steps/ibdne/{prefix}/s{seed}.ibd.gz",
        hbd="steps/ibdne/{prefix}/s{seed}.hbd.gz",
        ibd_postprocessed="steps/ibdne/{prefix}/s{seed}.postprocessed.ibd",
        pair_excl="steps/ibdne/{prefix}/s{seed}.pair.excl",
        region_excl="steps/ibdne/{prefix}/s{seed}.region.excl",
        estimate="steps/ibdne/{prefix}/s{seed}.ne",
        boot="steps/ibdne/{prefix}/s{seed}.boot",
    log:
        "logs/ibdne/{prefix}/s{seed}.log",
    shadow:
        "minimal"
    threads: 4
    resources:
        mem_mb=16000,
        runtime="2h",
    envmodules:
        "calcua/2024a",
        "BCFtools/1.21-GCC-13.3.0",
        "Java/21.0.5",
    params:
        recombination_rate=1e-8,
        gap=0.6,
        discord=1,
    shell:
        """
        echo "Starting IBDNE pipeline..." >> {log}
        echo "Concatenating input VCF files..." >> {log}
        bcftools concat {input.inbcfs} -O z > in.vcf.gz
        echo "Building plink genetic map..." >> {log}
        bcftools view --header-only in.vcf.gz | python {input.plink_map_script} {params.recombination_rate} > in.map
        echo "Running HapIBD..." >> {log}
        java -Xmx15g -jar {input.executable_hapibd} \
            gt=in.vcf.gz map=in.map out=steps/ibdne/{wildcards.prefix}/s{wildcards.seed} \
            nthreads={threads}
        cat steps/ibdne/{wildcards.prefix}/s{wildcards.seed}.log >> {log}
        echo "HapIBD completed" >> {log}
        echo "Post-processing IBD file..." >> {log}
        gunzip -c steps/ibdne/{wildcards.prefix}/s{wildcards.seed}.ibd.gz | \
            java -jar {input.executable_postprocess} in.vcf.gz in.map \
            {params.gap} {params.discord} > "steps/ibdne/{wildcards.prefix}/s{wildcards.seed}.postprocessed.ibd"
        echo "Post-processing completed" >> {log}
        echo "Running IBDNE..." >> {log}
        cat {output.ibd_postprocessed} | java -jar {input.executable_ibdne} out=out \
            seed={wildcards.seed} map=in.map nthreads={threads}
        cat *.log >> {log}
        mv *.pair.excl {output.pair_excl}
        mv *.region.excl {output.region_excl}
        mv *.ne {output.estimate}
        mv *.boot {output.boot}
        """


rule run_hapne_ld:
    input:
        intrees=expand(
            "steps/trees/{{prefix}}/s{{seed}}_chr{i}.trees", i=range(NUM_CHROMOSOMES)
        ),
        plink_tskit="src/utils/tskit_vcf_export.py",
        dir="external/hapne-snakemake",
    output:
        table="steps/hapne/{prefix}/s{seed}_ld_hapne_estimate.csv",
        summary="steps/hapne/{prefix}/s{seed}_ld_hapne_summary.txt",
        residuals="steps/hapne/{prefix}/s{seed}_ld_hapne_residuals.png",
        popsize="steps/hapne/{prefix}/s{seed}_ld_hapne_pop_trajectory.png",
    log:
        "logs/hapne_ld/{prefix}/s{seed}.log",
    shadow:
        "shallow"
    threads: 8
    resources:
        runtime="4h",
    params:
        rate=1e-8 * 100 * 1e6,
        sequence_length=int(1e8),
        cmlength=100,
    shell:
        """
        source {COMMON}
        cp -r {input.dir} hapne_analysis
        mkdir -p hapne_analysis/data
        # Process each input BCF file:
        for tree in {input.intrees}; do
            base=$(basename "$tree" .trees)
            chromosome=$(echo "$base" | sed 's/s{wildcards.seed}_//')
            echo "Processing $tree as $base" >> {log}
            python {input.plink_tskit} $tree $chromosome \
                | bcftools view -e 'POS=0' -O z \
                > hapne_analysis/data/$chromosome.vcf.gz
            echo "pposition rrate gposition" > hapne_analysis/data/$chromosome.shapeit.map
            echo "1 {params.rate} 0.0" >> hapne_analysis/data/$chromosome.shapeit.map
            echo "{params.sequence_length} {params.rate} {params.cmlength}" >> hapne_analysis/data/$chromosome.shapeit.map
        done
        cd hapne_analysis
        snakemake -c {threads} --config \
            data_dir='data/' out_dir='results/' \
            map_file_suffix='.shapeit.map' \
            plink_extra_flags='-chr-set 25 no-xy no-mt' \
            method='ld' 2>> ../{log}
        cd ..
        mv hapne_analysis/results/*_estimate.csv {output.table}
        mv hapne_analysis/results/*_summary.txt {output.summary}
        mv hapne_analysis/results/*_residuals.png {output.residuals}
        mv hapne_analysis/results/*_trajectory.png {output.popsize}
        """


rule run_hapne_ibd:
    input:
        intrees=expand(
            "steps/trees/{{prefix}}/s{{seed}}_chr{i}.trees", i=range(NUM_CHROMOSOMES)
        ),
        plink_tskit="src/utils/tskit_vcf_export.py",
        plink_map_script="src/utils/plink_map.py",
        dir="external/hapne-snakemake",
    output:
        table="steps/hapne/{prefix}/s{seed}_ibd_hapne_estimate.csv",
        summary="steps/hapne/{prefix}/s{seed}_ibd_hapne_summary.txt",
        residuals="steps/hapne/{prefix}/s{seed}_ibd_hapne_residuals.png",
        popsize="steps/hapne/{prefix}/s{seed}_ibd_hapne_pop_trajectory.png",
    log:
        "logs/hapne_ibd/{prefix}/s{seed}.log",
    shadow:
        "shallow"
    threads: 8
    resources:
        runtime="2h",
    envmodules:
        "calcua/2024a",
        "Java/21.0.5",
    params:
        recombination_rate=1e-8,
        sequence_length=int(1e8),
        cmlength=100,
    shell:
        """
        source {COMMON}
        cp -r {input.dir} hapne_analysis
        mkdir -p hapne_analysis/data
        echo "CHR\tFROM_BP\tTO_BP\tNAME\tLENGTH" > hapne_analysis/data/build.txt
        # Process each input BCF file:
        for tree in {input.intrees}; do
            base=$(basename "$tree" .trees)
            chromosome=$(echo "$base" | sed 's/s{wildcards.seed}_//')
            echo "Processing $tree as $base" >> {log}
            python {input.plink_tskit} $tree $chromosome \
                | bcftools view -e 'POS=0' -O z \
                > hapne_analysis/data/$chromosome.vcf.gz
            bcftools view --header-only \
                hapne_analysis/data/$chromosome.vcf.gz | \
                python {input.plink_map_script} {params.recombination_rate}\
                > hapne_analysis/data/$chromosome.plink.map
            echo "$chromosome\t0\t{params.sequence_length}\t$chromosome\t{params.cmlength}" >> hapne_analysis/data/build.txt
        done
        cd hapne_analysis
        cat data/build.txt >> ../{log}
        snakemake -c {threads} --config \
            data_dir='data/' out_dir='results/' \
            map_file_suffix='.plink.map' \
            method='ibd' genome_build='data/build.txt' 2>> ../{log}
        cd ..
        mv hapne_analysis/results/*_estimate.csv {output.table}
        mv hapne_analysis/results/*_summary.txt {output.summary}
        mv hapne_analysis/results/*_residuals.png {output.residuals}
        mv hapne_analysis/results/*_trajectory.png {output.popsize}
        """
